# -*- coding: utf-8 -*-
"""core_llm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/164wvN4QdfS-jl012KxAdxBZ89MZB2Oqi
"""

# core_llm.py
from typing import Optional, Dict, Any
from settings import LLM_PROVIDER, VERTEX_MODEL, VERTEX_REGION
from prompts import BASE_PROMPT  # reuse your template âœ…
import vertexai
from settings import PROJECT_ID, VERTEX_REGION
vertexai.init(project=PROJECT_ID, location=VERTEX_REGION)

def build_prompt(resume: str, jd: str, highlights: str,
                 length_style: str, format_style: str) -> str:
    # Exactly your placeholder keys from prompts.py
    return BASE_PROMPT.format(
        length_style=length_style,
        format_style=format_style,
        highlights=highlights,
        resume=resume,
        jd=jd,
    )

def generate_cover_letter(resume: str, jd: str, length_pref: str,
                          format_choice: str, highlights: str) -> str:
    prompt = build_prompt(resume, jd, highlights, length_pref, format_choice)

    if LLM_PROVIDER == "VERTEX":
        import vertexai
        from vertexai.generative_models import GenerativeModel
        vertexai.init(location=VERTEX_REGION)  # project inferred from ADC
        model = GenerativeModel(VERTEX_MODEL)
        out = model.generate_content(prompt)
        return (out.text or "").strip()

    raise ValueError(f"Unsupported LLM_PROVIDER={LLM_PROVIDER}")
