# -*- coding: utf-8 -*-
"""core_llm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/164wvN4QdfS-jl012KxAdxBZ89MZB2Oqi
"""

# core_llm.py
from typing import Optional, Dict, Any
from settings import LLM_PROVIDER, VERTEX_MODEL, VERTEX_REGION
from prompts import BASE_PROMPT, SUGGESTION_PROMPT
import vertexai
from settings import PROJECT_ID, VERTEX_REGION
vertexai.init(project=PROJECT_ID, location=VERTEX_REGION)
import vertexai
from vertexai.generative_models import GenerativeModel
        
def build_prompt_cover_letter(resume: str, jd: str, highlights: str,
                 length_style: str, format_style: str, feedback: Optional[str] = None) -> str:
    # Exactly your placeholder keys from prompts.py
    return BASE_PROMPT.format(
        length_style=length_style,
        format_style=format_style,
        highlights=highlights,
        resume=resume,
        jd=jd,
    )

def build_prompt_suggestion(resume: str, jd: str, highlights: str,
                 length_style: str, format_style: str, feedback: Optional[str] = None) -> str:
    # Exactly your placeholder keys from prompts.py
    return SUGGESTION_PROMPT.format(
        length_style=length_style,
        format_style=format_style,
        highlights=highlights,
        resume=resume,
        jd=jd,
    )

def generate_cover_letter(prompt: str) -> str:

    if LLM_PROVIDER == "VERTEX":
        vertexai.init(location=VERTEX_REGION)  # project inferred from ADC
        model = GenerativeModel(VERTEX_MODEL)
        out = model.generate_content(prompt)
        return (out.text or "").strip()

    # If you add openai to requirements later, you can enable this:
    # elif LLM_PROVIDER == "OPENAI":
    #     from openai import OpenAI
    #     client = OpenAI()
    #     resp = client.chat.completions.create(
    #         model=os.getenv("OPENAI_MODEL","gpt-4o-mini"),
    #         messages=[{"role":"system","content":"You write tailored cover letters."},
    #                   {"role":"user","content":prompt}],
    #         temperature=0.7,
    #     )
    #     return resp.choices[0].message.content.strip()
    
    raise ValueError(f"Unsupported LLM_PROVIDER={LLM_PROVIDER}")
